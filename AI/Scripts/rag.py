# ============================================
# üöÄ AGENT CONVERSATIONNEL SCIENTIFIQUE NASA
# ============================================

# ---- Imports principaux ----
import os
import pandas as pd
from dotenv import load_dotenv

# LangChain & Google Gemini
from langchain_google_genai import (
    GoogleGenerativeAIEmbeddings,
    ChatGoogleGenerativeAI
)
from langchain.document_loaders import AsyncHtmlLoader
from langchain.document_transformers import BeautifulSoupTransformer
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import FAISS
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory

# ---------------------------------------------
# 1Ô∏è‚É£ Charger la cl√© API Google depuis le fichier .env
# ---------------------------------------------
load_dotenv()
if not os.getenv("GOOGLE_API_KEY"):
    raise ValueError("‚ùå Cl√© API Google Gemini manquante dans le fichier .env !")
else:
    print("üîê Cl√© Google Gemini d√©tect√©e.")

# ---------------------------------------------
# 2Ô∏è‚É£ Charger les liens d‚Äôarticles depuis le CSV
# ---------------------------------------------

import os

# Construire le chemin relatif
csv_path = os.path.join("..", "..", "datas", "SB_publication_PMC.csv") 
df = pd.read_csv(csv_path)
links = df["Link"].tolist()
print(f"üîó Nombre total d'articles √† charger : {len(links)}")

# ---------------------------------------------
# 3Ô∏è‚É£ Charger les pages web en parall√®le (AsyncHtmlLoader)
# ---------------------------------------------
print("üì° T√©l√©chargement asynchrone des pages PMC en cours...")
loader = AsyncHtmlLoader(links)
docs = loader.load()
print(f"üì• Pages charg√©es : {len(docs)}")

# ---------------------------------------------
# 4Ô∏è‚É£ Nettoyer le contenu HTML (BeautifulSoupTransformer)
# ---------------------------------------------
print("üßπ Nettoyage du contenu HTML...")
bs_transformer = BeautifulSoupTransformer()
clean_docs = bs_transformer.transform_documents(docs, tags_to_extract=["p", "h1", "h2", "h3"])

# Ajouter la source (lien) dans les m√©tadonn√©es
for i, d in enumerate(clean_docs):
    d.metadata["source"] = links[i] if i < len(links) else None

print(f"üßæ Documents nettoy√©s : {len(clean_docs)}")

# ---------------------------------------------
# 5Ô∏è‚É£ D√©couper les textes en chunks (pour le RAG)
# ---------------------------------------------
print("‚úÇÔ∏è D√©coupage des textes en chunks...")
splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
chunks = splitter.split_documents(clean_docs)
print(f"üìö Nombre de chunks cr√©√©s : {len(chunks)}")

# ---------------------------------------------
# 6Ô∏è‚É£ Cr√©er les embeddings Gemini
# ---------------------------------------------
print("ü§ñ G√©n√©ration des embeddings avec Gemini...")
embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001")

# ---------------------------------------------
# 7Ô∏è‚É£ Cr√©er et sauvegarder l‚Äôindex FAISS
# ---------------------------------------------
print("üíæ Cr√©ation de l‚Äôindex FAISS...")
vectorstore = FAISS.from_documents(chunks, embeddings)
vectorstore.save_local("faiss_index_nasa_gemini")
print("‚úÖ Index vectoriel sauvegard√© sous : faiss_index_nasa_gemini")

# ---------------------------------------------
# 8Ô∏è‚É£ Charger l‚Äôindex et cr√©er le retriever
# ---------------------------------------------
print("üìÇ Chargement de l‚Äôindex FAISS...")
db = FAISS.load_local("faiss_index_nasa_gemini", embeddings, allow_dangerous_deserialization=True)
retriever = db.as_retriever(search_type="similarity", search_kwargs={"k": 5})

# ---------------------------------------------
# 9Ô∏è‚É£ Initialiser le mod√®le de conversation Gemini
# ---------------------------------------------
llm = ChatGoogleGenerativeAI(model="gemini-1.5-pro", temperature=0.3)
memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)

qa_chain = ConversationalRetrievalChain.from_llm(
    llm=llm,
    retriever=retriever,
    memory=memory,
    return_source_documents=True
)

# ---------------------------------------------
# üîü Exemple de conversation scientifique
# ---------------------------------------------
print("\nüß™ Assistant scientifique NASA pr√™t !")
print("Posez une question (ex : 'Quels sont les effets de la microgravit√© sur l‚ÄôADN ?')\n")

while True:
    query = input("‚ùì Votre question : ")
    if query.lower() in ["quit", "exit", "q"]:
        print("üëã Fin de la session.")
        break

    result = qa_chain({"question": query})
    print("\nüí¨ R√©ponse :")
    print(result["answer"])
    
    print("\nüìö Sources :")
    for doc in result["source_documents"]:
        print("-", doc.metadata.get("source"))
    print("\n" + "="*60 + "\n")
